{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPHA2bCg7Gr3R8Ee2YftwVh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ***Establishing Connection to Google Drive***\n","\n","To initiate the project, the primary step entails establishing a seamless connection to Google Drive. This connection is pivotal for accessing and utilizing the requisite files and datasets essential for the project's execution."],"metadata":{"id":"vHiYPcdHEf4A"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"QRlePOsDEKVy","executionInfo":{"status":"ok","timestamp":1716824717623,"user_tz":-180,"elapsed":38238,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}},"outputId":"f30136a4-3263-40de-ceb2-0bab3ede89bf","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# ***Importing Essential Libraries***\n","\n","Following the establishment of the Google Drive connection, the subsequent step involves importing the essential libraries necessary for executing the code. These libraries serve as the foundational framework, providing the functionality and tools required to implement various tasks and analyses within the project."],"metadata":{"id":"kZ1MOzL4Eiqn"}},{"cell_type":"code","source":["import os\n","import librosa\n","import numpy as np\n","\n","from sklearn.ensemble import IsolationForest\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.metrics import average_precision_score, confusion_matrix"],"metadata":{"id":"axncO02FElWW","executionInfo":{"status":"ok","timestamp":1716824729606,"user_tz":-180,"elapsed":1606,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# ***Loading Data for Processing and Testing***\n","\n","In this pivotal stage, we load all pertinent data into the project environment for comprehensive processing and testing. By importing the datasets integral to our analysis, we ensure a robust foundation for conducting experiments and evaluations crucial to the project's objectives."],"metadata":{"id":"p4hCSjY0FRzi"}},{"cell_type":"code","source":["# for 1 second vggish\n","pod_train_embeddings = np.load(\"/content/drive/MyDrive/AD-Blocker Project/embeddings/vggish_10sec/vggish_10sec_pod_train_embeddings.npy\")\n","# [:502, :, :]\n","ad_train_embeddings = np.load(\"/content/drive/MyDrive/AD-Blocker Project/embeddings/vggish_10sec/vggish_10sec_ad_train_embeddings.npy\")\n","\n","pod_val_embeddings = np.load(\"/content/drive/MyDrive/AD-Blocker Project/embeddings/vggish_10sec/vggish_10sec_pod_val_embeddings.npy\")\n","ad_val_embeddings = np.load(\"/content/drive/MyDrive/AD-Blocker Project/embeddings/vggish_10sec/vggish_10sec_ad_val_embeddings.npy\")\n","\n","pod_test_embeddings = np.load(\"/content/drive/MyDrive/AD-Blocker Project/embeddings/vggish_10sec/vggish_10sec_pod_test_embeddings.npy\")\n","ad_test_embeddings = np.load(\"/content/drive/MyDrive/AD-Blocker Project/embeddings/vggish_10sec/vggish_10sec_ad_test_embeddings.npy\")\n","\n","# load all relevant data for the ads\n","#ad_train_embeddings = np.load('need to add the actual path to the relevant')\n","#ad_train_labels = np.load('need to add the actual path to the relevant')\n","#ad_val_embeddings = np.load('need to add the actual path to the relevant')\n","#ad_val_labels = np.load('need to add the actual path to the relevant')\n","#ad_test_embeddings = np.load('need to add the actual path to the relevant')\n","#ad_ test_labels = np.load('need to add the actual path to the relevant')"],"metadata":{"id":"xj5CzFiFE_ld","executionInfo":{"status":"ok","timestamp":1716828036584,"user_tz":-180,"elapsed":3193,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}}},"execution_count":110,"outputs":[]},{"cell_type":"code","source":["print(pod_train_embeddings.shape)\n","print(ad_train_embeddings.shape)\n","\n","print(pod_val_embeddings.shape)\n","print(ad_val_embeddings.shape)\n","\n","print(pod_test_embeddings.shape)\n","print(ad_test_embeddings.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fnnd72yyNHjX","executionInfo":{"status":"ok","timestamp":1716828038725,"user_tz":-180,"elapsed":387,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}},"outputId":"db0bfb05-9641-40f2-eb60-0e6fa359817a"},"execution_count":111,"outputs":[{"output_type":"stream","name":"stdout","text":["(1639, 10, 128)\n","(240, 10, 128)\n","(204, 10, 128)\n","(30, 10, 128)\n","(206, 10, 128)\n","(30, 10, 128)\n"]}]},{"cell_type":"code","source":["def convert_to_2d_array(three_d_array):\n","    # Get the dimensions of the input array\n","    depth, rows, cols = three_d_array.shape\n","\n","    # Reshape each 2D array to 1D and concatenate them\n","    flattened_arrays = [matrix.flatten() for matrix in three_d_array]\n","    two_d_array = np.vstack(flattened_arrays)\n","\n","    return two_d_array"],"metadata":{"id":"eBQkFj28NH5O","executionInfo":{"status":"ok","timestamp":1716828054981,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}}},"execution_count":112,"outputs":[]},{"cell_type":"code","source":["pod_train_embeddings = convert_to_2d_array(pod_train_embeddings)\n","ad_train_embeddings = convert_to_2d_array(ad_train_embeddings)\n","\n","pod_val_embeddings = convert_to_2d_array(pod_val_embeddings)\n","ad_val_embeddings = convert_to_2d_array(ad_val_embeddings)\n","\n","pod_test_embeddings =  convert_to_2d_array(pod_test_embeddings)\n","ad_test_embeddings = convert_to_2d_array(ad_test_embeddings)\n","\n","print(pod_train_embeddings.shape)\n","print(ad_train_embeddings.shape)\n","\n","print(pod_val_embeddings.shape)\n","print(ad_val_embeddings.shape)\n","\n","print(pod_test_embeddings.shape)\n","print(ad_test_embeddings.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T2FN-ralNKMk","executionInfo":{"status":"ok","timestamp":1716828056546,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}},"outputId":"f7fffa2f-1564-40c3-d20b-b76ef7e7a82a"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["(1639, 1280)\n","(240, 1280)\n","(204, 1280)\n","(30, 1280)\n","(206, 1280)\n","(30, 1280)\n"]}]},{"cell_type":"code","source":["train_embeddings = np.concatenate((pod_train_embeddings, ad_train_embeddings))\n","val_embeddings = np.concatenate((pod_val_embeddings, ad_val_embeddings))\n","test_embeddings = np.concatenate((pod_test_embeddings, ad_test_embeddings))"],"metadata":{"id":"8ILnX8PTNMQH","executionInfo":{"status":"ok","timestamp":1716828059500,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}}},"execution_count":114,"outputs":[]},{"cell_type":"code","source":["print(train_embeddings.shape)\n","print(val_embeddings.shape)\n","print(test_embeddings.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"05_pcz7WdFuZ","executionInfo":{"status":"ok","timestamp":1716828061657,"user_tz":-180,"elapsed":278,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}},"outputId":"168e7e50-a2a5-471c-98a6-789c5036045d"},"execution_count":115,"outputs":[{"output_type":"stream","name":"stdout","text":["(1879, 1280)\n","(234, 1280)\n","(236, 1280)\n"]}]},{"cell_type":"markdown","source":["# ***Training Isolation Forest Model***\n","\n","In this critical phase, we commence the training process for our  Isolation Forest model by executing the data allocated to the training set. Prior to this, we crafted a function designed to furnish us with the requisite model. Subsequently, invoking this function enables us to obtain the model tailored to our specifications, facilitating the subsequent stages of our analysis."],"metadata":{"id":"5GTVIZ7wFlwi"}},{"cell_type":"code","source":["def train_isolation_forest_model(normal_embeddings, contamination_rate=0.05):\n","    \"\"\"\n","    Train an Isolation Forest model on normal data points.\n","\n","    Parameters:\n","    ----------\n","    normal_embeddings (numpy.ndarray): Array containing normal data points.\n","    contamination_rate (float, optional): Contamination rate for the Isolation\n","    Forest model.  Defaults to 0.05 (5%).\n","\n","    Returns:\n","    -------\n","    sklearn.ensemble.IsolationForest: Trained Isolation Forest model.\n","    \"\"\"\n","    # Initialize Isolation Forest model with the specified contamination rate\n","    contamination='auto'\n","    isolation_forest_model = IsolationForest(contamination='auto')\n","    # isolation_forest_model = IsolationForest(contamination=contamination_rate)\n","\n","    # Fit Isolation Forest model to normal data points\n","    isolation_forest_model.fit(normal_embeddings)\n","\n","    return isolation_forest_model"],"metadata":{"id":"7rVIKRVTFk_u","executionInfo":{"status":"ok","timestamp":1716828065624,"user_tz":-180,"elapsed":278,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}}},"execution_count":116,"outputs":[]},{"cell_type":"code","source":["Isolation_Forest_trained_model = train_isolation_forest_model(train_embeddings, contamination_rate=0.13) # can Change the contamination rate if needed"],"metadata":{"id":"nvqQqD1HGnUq","executionInfo":{"status":"ok","timestamp":1716828071461,"user_tz":-180,"elapsed":317,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}}},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":["# ***Model Validation and Adjustment***\n","\n","In this pivotal stage, we evaluate the performance of our model by running the data assigned to the validation set. This step enables us to conduct a thorough examination of the model's efficacy and identify any necessary adjustments. By scrutinizing the model's performance against validation data, we iteratively refine its parameters to enhance its accuracy and robustness."],"metadata":{"id":"IJrp9jqYHbl0"}},{"cell_type":"code","source":["def predict_anomalies(isolation_forest_model, data_points):\n","    \"\"\"\n","    Predict anomalies using the trained Isolation Forest model.\n","\n","    Parameters:\n","    ---------\n","    isolation_forest_model (sklearn.ensemble.IsolationForest): Trained Isolation Forest model.\n","    data_points (numpy.ndarray): Array containing data points to be predicted.\n","\n","    Returns:\n","    -------\n","    numpy.ndarray: Predicted labels indicating anomalies (1 for anomalies, -1 for normal data points).\n","    \"\"\"\n","    # Predict anomalies using Isolation Forest model\n","    anomaly_labels = isolation_forest_model.predict(data_points)\n","\n","    return anomaly_labels"],"metadata":{"id":"YWMx5ulUHdRX","executionInfo":{"status":"ok","timestamp":1716828073935,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}}},"execution_count":118,"outputs":[]},{"cell_type":"code","source":["# Assuming you have a trained Isolation Forest model named 'trained_model' and data points stored in a numpy array named 'data_points'\n","\n","# Call the predict_anomalies function\n","val_predicted_labels = predict_anomalies(Isolation_Forest_trained_model, val_embeddings)\n","print(val_predicted_labels.shape)"],"metadata":{"id":"KTXmhSPBIW0x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716828076707,"user_tz":-180,"elapsed":437,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}},"outputId":"3ac23099-d672-49b2-b406-d5df261c8535"},"execution_count":119,"outputs":[{"output_type":"stream","name":"stdout","text":["(234,)\n"]}]},{"cell_type":"markdown","source":["# ***Final Model Evaluation with Test Data***\n","\n","In this culminating phase, we subject the test data to our final model for comprehensive evaluation of its performance on real-world datasets. This step allows us to ascertain the effectiveness and generalizability of our model beyond the training and validation stages. By rigorously scrutinizing the model's performance against unseen data, we derive insights into its real-world applicability and overall efficacy."],"metadata":{"id":"3c71Hr0SIia4"}},{"cell_type":"code","source":["test_predicted_labels =  predict_anomalies(Isolation_Forest_trained_model, test_embeddings)\n","print(test_predicted_labels.shape)"],"metadata":{"id":"l9wGSIN5IiIA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716828080531,"user_tz":-180,"elapsed":263,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}},"outputId":"ca078446-8fe5-4203-8981-e4b247823576"},"execution_count":120,"outputs":[{"output_type":"stream","name":"stdout","text":["(236,)\n"]}]},{"cell_type":"markdown","source":["# ***Label Transformation for Model Evaluation***\n","\n","At this critical stage, we undertake the transformation of label arrays to facilitate the evaluation process of our model. Specifically, we convert the labels representing podcasts into \"-1\" and those denoting anomalies (advertisements) into \"1\". This transformation enables a standardized evaluation framework, enhancing the model's interpretability and facilitating the assessment of its performance metrics with clarity and precision.\n"],"metadata":{"id":"SU_kW3gVJi8T"}},{"cell_type":"code","source":["# def concatenate_ones_and_minus_one(podcast_length, commercials_length):\n","#     # Create array of zeros with size of podcast array\n","#     zeros_array = np.ones(podcast_length, dtype=int)\n","\n","#     # Create array of ones with size of commercials array\n","#     # ones_array = np.ones(commercials_length, dtype=int)\n","#     minus_ones_array = np.full(commercials_length, -1, dtype=int)\n","#     # Concatenate arrays\n","#     concatenated_array = np.concatenate((zeros_array, minus_ones_array))\n","\n","#     return concatenated_array\n","\n","\n","def concatenate_zeros_and_ones(podcast_length, commercials_length):\n","    # Create array of zeros with size of podcast array\n","    zeros_array = np.full(podcast_length, 0, dtype=int)\n","\n","    # Create array of ones with size of commercials array\n","    # ones_array = np.ones(commercials_length, dtype=int)\n","    ones_array = np.ones(commercials_length, dtype=int)\n","    # Concatenate arrays\n","    concatenated_array = np.concatenate((zeros_array, ones_array))\n","\n","    return concatenated_array\n","\n","\n","\n","\n","podcast_length = pod_train_embeddings.shape[0]\n","ads_length = ad_train_embeddings.shape[0]"],"metadata":{"id":"WpGWqYVnZmBE","executionInfo":{"status":"ok","timestamp":1716828083357,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}}},"execution_count":121,"outputs":[]},{"cell_type":"code","source":["train_labels = concatenate_zeros_and_ones(pod_train_embeddings.shape[0], ad_train_embeddings.shape[0])\n","val_labels = concatenate_zeros_and_ones(pod_val_embeddings.shape[0], ad_val_embeddings.shape[0])\n","test_lables = concatenate_zeros_and_ones(pod_test_embeddings.shape[0], ad_test_embeddings.shape[0])"],"metadata":{"id":"sRb7o3xIZoKc","executionInfo":{"status":"ok","timestamp":1716828085861,"user_tz":-180,"elapsed":277,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}}},"execution_count":122,"outputs":[]},{"cell_type":"code","source":["# def convert_normal_to_minus_one(samples):\n","#     \"\"\"\n","#     Convert elements equal to 0 to -1 in the samples.\n","\n","#     Parameters:\n","#     samples (list of numpy.ndarray): List of one-dimensional numpy arrays containing zeros or ones.\n","\n","#     Returns:\n","#     list of numpy.ndarray: List of samples with 0 replaced by -1.\n","#     \"\"\"\n","#     converted_samples = []\n","#     for sample in samples:\n","#         converted_sample = np.where(sample == 1, -1, sample)\n","#         converted_samples.append(converted_sample)\n","#     return converted_samples\n","\n","# def convert_anomalies_from_1_to_minus_1(samples):\n","#     \"\"\"\n","#     Convert elements equal to 0 to -1 in the samples.\n","\n","#     Parameters:\n","#     samples (list of numpy.ndarray): List of one-dimensional numpy arrays containing zeros or ones.\n","\n","#     Returns:\n","#     list of numpy.ndarray: List of samples with 0 replaced by -1.\n","#     \"\"\"\n","#     converted_samples = []\n","#     for sample in samples:\n","#         converted_sample = np.where(sample == 1, -1, sample)\n","#         converted_samples.append(converted_sample)\n","#     return converted_samples\n","\n","# def convert_podcasts_from_0_to_1(samples):\n","#     \"\"\"\n","#     Convert elements equal to 0 to -1 in the samples.\n","\n","#     Parameters:\n","#     samples (list of numpy.ndarray): List of one-dimensional numpy arrays containing zeros or ones.\n","\n","#     Returns:\n","#     list of numpy.ndarray: List of samples with 0 replaced by -1.\n","#     \"\"\"\n","#     converted_samples = []\n","#     for sample in samples:\n","#         converted_sample = np.where(sample == 1, -1, sample)\n","#         converted_samples.append(converted_sample)\n","#     return converted_samples"],"metadata":{"id":"CV8MSVsiJjSr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Call the function to convert the val_labels and test_labels to be array pf\n","# 1 and -1 so we can evaluate the model\n","#val_converted_labels = convert_normal_to_minus_one(val_labels) # validation labels\n","#test_converted_labels = convert_normal_to_minus_one(test_lables) # test lables\n","\n","# val_converted_labels = convert_anomalies_from_1_to_minus_1(val_labels) # validation labels\n","# test_converted_labels = convert_anomalies_from_1_to_minus_1(test_lables) # test lables\n","\n","# val_converted_labels = convert_podcasts_from_0_to_1(val_labels) # validation labels\n","# test_converted_labels = convert_podcasts_from_0_to_1(test_lables) # test lables\n"],"metadata":{"id":"JdV9B5WxKouR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***Presentation of Model Evaluation Metrics***\n","\n","In this critical juncture, we showcase a comprehensive array of evaluation metrics meticulously designed to gauge the efficacy and performance of our model. Through the presentation of these metrics, including but not limited to accuracy, precision, recall, and F1-score, we offer a nuanced understanding of the model's strengths and limitations. This holistic evaluation serves to validate the model's efficacy and aids in informing future iterations or enhancements."],"metadata":{"id":"9MD-h31hLoX_"}},{"cell_type":"code","source":["# Convert predictions: -1 (anomaly) to 1, 1 (normal) to 0\n","test_predicted_labels_binary = np.where(test_predicted_labels == -1, 1, 0)\n","\n","# Compute the confusion matrix\n","cm = confusion_matrix(test_lables, test_predicted_labels_binary)\n","print(\"Confusion Matrix:\\n\", cm)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E4dRr2Bky6C7","executionInfo":{"status":"ok","timestamp":1716828091432,"user_tz":-180,"elapsed":334,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}},"outputId":"2683d47c-3fca-4113-80c6-b6dece0d8f4d"},"execution_count":123,"outputs":[{"output_type":"stream","name":"stdout","text":["Confusion Matrix:\n"," [[204   2]\n"," [ 29   1]]\n"]}]},{"cell_type":"code","source":["def calculate_confusion_matrix(actual_labels, predicted_labels):\n","    \"\"\"\n","    Calculate the confusion matrix based on actual and predicted labels.\n","\n","    Parameters:\n","    actual_labels (list): List of actual labels (-1 for ads, 1 for podcasts).\n","    predicted_labels (list): List of predicted labels (-1 for ads, 1 for podcasts).\n","\n","    Returns:\n","    tuple: A tuple containing true positives, false positives, true negatives, and false negatives.\n","    \"\"\"\n","    true_positives = 0\n","    false_positives = 0\n","    true_negatives = 0\n","    false_negatives = 0\n","\n","    for actual, predicted in zip(actual_labels, predicted_labels):\n","        if actual == 0 and predicted == 0:  # true_negatives\n","            true_negatives += 1\n","        elif actual == 1 and predicted == 0:  # false_negatives\n","            false_negatives += 1\n","        elif actual == 1 and predicted == 1:  # true_positives\n","            true_positives += 1\n","        elif actual == 0 and predicted == 1:  # false_positives\n","            false_positives += 1\n","\n","    return true_positives, false_positives, true_negatives, false_negatives\n","# Example usage:\n","true_positives, false_positives, true_negatives, false_negatives = calculate_confusion_matrix(test_lables, test_predicted_labels_binary)\n","print(\"True Positives:\", true_positives)\n","print(\"False Positives:\", false_positives)\n","print(\"True Negatives:\", true_negatives)\n","print(\"False Negatives:\", false_negatives)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6qcZ-lT1MCK","executionInfo":{"status":"ok","timestamp":1716828102963,"user_tz":-180,"elapsed":273,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}},"outputId":"a9ef7f73-8be8-4c35-f254-94c4bb096617"},"execution_count":124,"outputs":[{"output_type":"stream","name":"stdout","text":["True Positives: 1\n","False Positives: 2\n","True Negatives: 204\n","False Negatives: 29\n"]}]},{"cell_type":"code","source":["# def calculate_confusion_matrix(actual_labels, predicted_labels):\n","#     \"\"\"\n","#     Calculate the confusion matrix based on actual and predicted labels.\n","\n","#     Parameters:\n","#     actual_labels (list): List of actual labels (-1 for ads, 1 for podcasts).\n","#     predicted_labels (list): List of predicted labels (-1 for ads, 1 for podcasts).\n","\n","#     Returns:\n","#     tuple: A tuple containing true positives, false positives, true negatives, and false negatives.\n","#     \"\"\"\n","#     true_positives = 0\n","#     false_positives = 0\n","#     true_negatives = 0\n","#     false_negatives = 0\n","\n","#     for actual, predicted in zip(actual_labels, predicted_labels):\n","#         if actual == -1 and predicted == -1:  # True positive\n","#             true_positives += 1\n","#         elif actual == 1 and predicted == -1:  # False positive\n","#             false_positives += 1\n","#         elif actual == 1 and predicted == 1:  # True negative\n","#             true_negatives += 1\n","#         elif actual == -1 and predicted == 1:  # False negative\n","#            false_negatives += 1\n","#\n","#    return true_positives, false_positives, true_negatives, false_negatives\n","\n","def calculate_confusion_matrix(actual_labels, predicted_labels):\n","    \"\"\"\n","    Calculate the confusion matrix based on actual and predicted labels.\n","\n","    Parameters:\n","    actual_labels (list): List of actual labels (-1 for ads, 1 for podcasts).\n","    predicted_labels (list): List of predicted labels (-1 for ads, 1 for podcasts).\n","\n","    Returns:\n","    tuple: A tuple containing true positives, false positives, true negatives, and false negatives.\n","    \"\"\"\n","    true_positives = 0\n","    false_positives = 0\n","    true_negatives = 0\n","    false_negatives = 0\n","\n","    for actual, predicted in zip(actual_labels, predicted_labels):\n","        if actual == 1 and predicted == 1:  # True positive\n","            true_positives += 1\n","        elif actual == -1 and predicted == 1:  # False positive\n","            false_positives += 1\n","        elif actual == -1 and predicted == -1:  # True negative\n","            true_negatives += 1\n","        elif actual == 1 and predicted == -1:  # False negative\n","            false_negatives += 1\n","\n","    return true_positives, false_positives, true_negatives, false_negatives\n","\n","# Example usage:\n","true_positives, false_positives, true_negatives, false_negatives = calculate_confusion_matrix(test_lables, test_predicted_labels)\n","print(\"True Positives:\", true_positives)\n","print(\"False Positives:\", false_positives)\n","print(\"True Negatives:\", true_negatives)\n","print(\"False Negatives:\", false_negatives)\n","\n","# Compute the confusion matrix\n","cm = confusion_matrix(test_lables, test_predicted_labels)\n","print(\"Confusion Matrix:\\n\", cm)"],"metadata":{"id":"iuPyXh-mLo28","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716825629709,"user_tz":-180,"elapsed":289,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}},"outputId":"0bda847a-14ba-439f-b376-7b45108792a6"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["True Positives: 1997\n","False Positives: 297\n","True Negatives: 26\n","False Negatives: 60\n","Confusion Matrix:\n"," [[  26  297]\n"," [  60 1997]]\n"]}]},{"cell_type":"code","source":["def calculate_accuracy(actual_labels, predicted_labels):\n","    \"\"\"\n","    Calculate the accuracy of the predictions.\n","\n","    Parameters:\n","    actual_labels (list): List of actual labels (-1 for ads, 1 for podcasts).\n","    predicted_labels (list): List of predicted labels (-1 for ads, 1 for podcasts).\n","\n","    Returns:\n","    float: Accuracy of the predictions.\n","    \"\"\"\n","    correct_predictions = sum(1 for actual, predicted in zip(actual_labels, predicted_labels) if actual == predicted)\n","    total_predictions = len(actual_labels)\n","    accuracy = correct_predictions / total_predictions\n","    return accuracy\n","\n","\n","def calculate_precision(actual_labels, predicted_labels):\n","    \"\"\"\n","    Calculate the precision of the predictions.\n","\n","    Parameters:\n","    actual_labels (list): List of actual labels (-1 for ads, 1 for podcasts).\n","    predicted_labels (list): List of predicted labels (-1 for ads, 1 for podcasts).\n","\n","    Returns:\n","    float: Precision of the predictions.\n","    \"\"\"\n","    true_positives, false_positives, _, _ = calculate_confusion_matrix(actual_labels, predicted_labels)\n","    precision = true_positives / (true_positives + false_positives)\n","    return precision\n","\n","\n","def calculate_recall(actual_labels, predicted_labels):\n","    \"\"\"\n","    Calculate the recall of the predictions.\n","\n","    Parameters:\n","    actual_labels (list): List of actual labels (-1 for ads, 1 for podcasts).\n","    predicted_labels (list): List of predicted labels (-1 for ads, 1 for podcasts).\n","\n","    Returns:\n","    float: Recall of the predictions.\n","    \"\"\"\n","    true_positives, _, _, false_negatives = calculate_confusion_matrix(actual_labels, predicted_labels)\n","    recall = true_positives / (true_positives + false_negatives)\n","    return recall\n","\n","\n","def calculate_f1_score(actual_labels, predicted_labels):\n","    \"\"\"\n","    Calculate the F1 score of the predictions.\n","\n","    Parameters:\n","    actual_labels (list): List of actual labels (-1 for ads, 1 for podcasts).\n","    predicted_labels (list): List of predicted labels (-1 for ads, 1 for podcasts).\n","\n","    Returns:\n","    float: F1 score of the predictions.\n","    \"\"\"\n","    precision = calculate_precision(actual_labels, predicted_labels)\n","    recall = calculate_recall(actual_labels, predicted_labels)\n","    f1_score = 2 * (precision * recall) / (precision + recall)\n","    return f1_score"],"metadata":{"id":"X7l6XZVLNcIp","executionInfo":{"status":"ok","timestamp":1716825323101,"user_tz":-180,"elapsed":381,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["# Assuming you have actual labels stored in 'true_test_label' and predicted\n","# labels stored in 'anomaly_predictions_test'\n","\n","accuracy = calculate_accuracy(test_lables, test_predicted_labels)\n","precision = calculate_precision(test_lables, test_predicted_labels)\n","recall = calculate_recall(test_lables, test_predicted_labels)\n","f1_score = calculate_f1_score(test_lables, test_predicted_labels)\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1 Score:\", f1_score)"],"metadata":{"id":"Vro-opfANfld","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716825325217,"user_tz":-180,"elapsed":7,"user":{"displayName":"Ariel Erusalimsky","userId":"09307154366084279372"}},"outputId":"71066fff-1a23-4c63-b887-5c5460f49c3a"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.85\n","Precision: 0.8705318221447254\n","Recall: 0.9708313077297035\n","F1 Score: 0.9179498965754999\n"]}]}]}